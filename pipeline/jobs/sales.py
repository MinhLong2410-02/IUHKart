import json
from pyflink.common import Types, Time
from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext
from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig
from utils import initialize_env, configure_source, get_clickhouse_client, decode_message
import logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())

class DeduplicationProcessFunction(KeyedProcessFunction):
    """ProcessFunction ƒë·ªÉ th·ª±c hi·ªán deduplication d·ª±a tr√™n review_id."""

    def __init__(self):
        self.review_id_state = None
        self.clickhouse_client = None

    def open(self, runtime_context: RuntimeContext):
        # Thi·∫øt l·∫≠p state descriptor v·ªõi TTL 1 ng√†y
        
        ttl_config = (
            StateTtlConfig
            .new_builder(Time.days(1))
            .set_update_type(StateTtlConfig.UpdateType.OnCreateAndWrite)
            .build()
        )
        descriptor = ValueStateDescriptor("review_id_state", Types.BOOLEAN())
        descriptor.enable_time_to_live(ttl_config)
        self.review_id_state = runtime_context.get_state(descriptor)

        # Kh·ªüi t·∫°o ClickHouse client
        self.clickhouse_client = get_clickhouse_client()

    def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):
        """
        X·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠ trong stream.
        value: Chu·ªói JSON t·ª´ Kafka
        """
        mode, data = decode_message(value)
        if not data:
            logger.warning("No data extracted from message.")
            return

        review_id = data.get('review_id')
        if not review_id:
            logger.warning("No review_id found in message.")
            return

        # Ki·ªÉm tra xem review_id ƒë√£ t·ªìn t·∫°i trong state ch∆∞a
        seen = self.review_id_state.value()
        if seen:
            # ƒê√£ th·∫•y trong state, b·ªè qua
            logger.info(f"Duplicate review_id {review_id} found in state. Skipping.")
            return

        # N·∫øu ch∆∞a th·∫•y trong state, ki·ªÉm tra trong ClickHouse
        try:
            query = f"SELECT COUNT(*) FROM fact_review WHERE id = %(id)s"
            result = self.clickhouse_client.query(query, {"id": review_id})
            count = result.result_set[0][0]
            if count > 0:
                logger.info(f"Duplicate review_id {review_id} found in ClickHouse. Skipping.")
                return
        except Exception as e:
            logger.error(f"Error querying ClickHouse for review_id {review_id}: {e}")
            return

        # N·∫øu ch∆∞a t·ªìn t·∫°i, ch√®n v√†o ClickHouse v√† c·∫≠p nh·∫≠t state
        try:
            content = data.get('review_content')
            rating = data.get('review_rating')
            date_id = data.get('review_date')
            product_id = data.get('product_id_id')
            customer_id = data.get('customer_id_id')

            row = (review_id, content, rating, date_id, product_id, customer_id)

            self.clickhouse_client.insert(
                'fact_review',
                [row],
                column_names=['id', 'content', 'rating', 'date_id', 'product_id', 'customer_id']
            )

            # C·∫≠p nh·∫≠t state ƒë·ªÉ ƒë√°nh d·∫•u review_id ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
            self.review_id_state.update(True)

            logger.info(f"üü¢ Inserted into ClickHouse: {row}")

        except Exception as e:
            logger.error(f"‚ùå Error inserting review_id {review_id} into ClickHouse: {e}")

    def close(self):
        if self.clickhouse_client:
            self.clickhouse_client.close()

def main() -> None:
    """Main flow controller"""

    # Initialize environment
    env = initialize_env()
    logger.info("üü¢ Initializing environment")

    # Define source
    kafka_source = configure_source(f"{KAFKA_HOST}:{KAFKA_PORT}", KAFKA_HEAD_TOPIC)
    logger.info("üü¢ Configuring Kafka source")

    data_stream = env.from_source(
        kafka_source, WatermarkStrategy.no_watermarks(), "Kafka reviews topic"
    )
    logger.info("üü¢ Created DataStream from Kafka source")

    # √Åp d·ª•ng deduplication
    dedup_stream = (
        data_stream
        .key_by(lambda msg: json.loads(msg).get('review_id'))
        .process(DeduplicationProcessFunction(), output_type=Types.VOID())
    )
    logger.info("üü¢ Applied deduplication ProcessFunction")

    # Th·ª±c thi job Flink
    env.execute("Flink ETL Job with Deduplication")

if __name__ == "__main__":
    main()